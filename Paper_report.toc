\contentsline {part}{I\hspace {1em}Paper report}{4}
\contentsline {section}{\numberline {1}Unsupervised Neural HMMs \citep {Tran16unsupervised}}{4}
\contentsline {subsection}{\numberline {1.1}Main idea}{4}
\contentsline {subsection}{\numberline {1.2}What is their concentration ?}{4}
\contentsline {subsection}{\numberline {1.3}What is the role of HMM ?}{5}
\contentsline {subsection}{\numberline {1.4}Where is neural network ?}{5}
\contentsline {section}{\numberline {2}What does Attention in NMT pay Attention to ? \citep {Ghader2017what} }{6}
\contentsline {subsection}{\numberline {2.1}Main idea}{6}
\contentsline {subsection}{\numberline {2.2}How to compare ?}{6}
\contentsline {subsection}{\numberline {2.3}Analysis}{7}
\contentsline {section}{\numberline {3}Confidence through Attention \citep {Rikters2017confidence}}{7}
\contentsline {subsection}{\numberline {3.1}Main idea}{7}
\contentsline {subsection}{\numberline {3.2}How to calculate Confidence metric ?}{8}
\contentsline {subsection}{\numberline {3.3}Analysis}{8}
\contentsline {subsubsection}{\numberline {3.3.1}Comparison with human evaluation}{8}
\contentsline {subsubsection}{\numberline {3.3.2}Exp: Filtering Back-translated Data}{8}
\contentsline {subsubsection}{\numberline {3.3.3}Exp: Hybrid Decisions}{9}
\contentsline {section}{\numberline {4}Word Translation without Parallel data \citep {Conneau2017Word}}{9}
\contentsline {subsection}{\numberline {4.1}Main idea}{9}
\contentsline {subsection}{\numberline {4.2}How does it works ?}{9}
\contentsline {section}{\numberline {5}Alignment by Agreement \citep {Liang2006Alignment}}{10}
\contentsline {subsection}{\numberline {5.1}Main idea}{10}
\contentsline {subsection}{\numberline {5.2}How to train these two model ?}{10}
\contentsline {subsubsection}{\numberline {5.2.1}Optimization via EM}{11}
\contentsline {subsection}{\numberline {5.3}Analysis}{12}
\contentsline {section}{\numberline {6}Word Alignment Modeling with Context Dependent Deep Neural Network \citep {Yang13word}}{12}
\contentsline {subsection}{\numberline {6.1}Main idea}{12}
\contentsline {subsection}{\numberline {6.2}How does DNN work in word alignment ?}{12}
\contentsline {subsection}{\numberline {6.3}Loss in supervised learning}{13}
\contentsline {subsection}{\numberline {6.4}Analysis}{13}
\contentsline {subsubsection}{\numberline {6.4.1}Two loss functions during training}{13}
\contentsline {subsubsection}{\numberline {6.4.2}Result}{14}
\contentsline {subsubsection}{\numberline {6.4.3}Notes}{14}
\contentsline {section}{\numberline {7}Recurrent Neural Networks for Word Alignment Model \citep {Tamura14recurrent}}{14}
\contentsline {subsection}{\numberline {7.1}Main idea}{14}
\contentsline {subsection}{\numberline {7.2}How does RNNs work in calculating alignment score ?}{14}
\contentsline {subsection}{\numberline {7.3}Loss in unsupervised learning}{14}
\contentsline {subsection}{\numberline {7.4}Agreement constraints}{15}
\contentsline {subsection}{\numberline {7.5}Analysis}{15}
\contentsline {subsubsection}{\numberline {7.5.1}Result}{16}
\contentsline {section}{\numberline {8}Hierarchical Multiscale Recurrent Neural Networks \citep {Chung2016Hierarchical}}{16}
\contentsline {subsection}{\numberline {8.1}Main idea}{16}
\contentsline {subsection}{\numberline {8.2}Update mechanism}{17}
\contentsline {subsection}{\numberline {8.3}Analysis}{18}
\contentsline {section}{\numberline {9}Alignment-based Neural Machine Translation \citep {Alkhouli2016AlignmentBasedNM} }{18}
\contentsline {section}{\numberline {10}Hybrid Neural Network alignment and Lexicon Model in direct HMM for Statistical Machine Translation \citep {Wang2017HybridNN} }{18}
\contentsline {subsection}{\numberline {10.1}NN-based Alignment and Lexical model}{18}
\contentsline {subsection}{\numberline {10.2}Objective function}{19}
\contentsline {subsection}{\numberline {10.3}Algorithm}{19}
\contentsline {section}{\numberline {11}HMM Word and Phrase Alignment for Statistical Machine Translation \citep {Deng2008HMM} }{19}
\contentsline {section}{\numberline {12}Neural Network-based Word Alignment through Score Aggregation \citep {Legrand16neural}}{19}
\contentsline {part}{II\hspace {1em}Report}{20}
\contentsline {section}{\numberline {13}Overview about statistical alignment }{20}
\contentsline {subsection}{\numberline {13.1}Word-based alignment ? \cite {Och2003Systematic}}{20}
\contentsline {subsection}{\numberline {13.2}Hidden Markov Alignment Model \cite {Och2003Systematic}}{20}
\contentsline {subsubsection}{\numberline {13.2.1}Assumption about HMM alignment probability $p(a_j | a_{j-1}, I)$ or $p(i | i', I)$}{21}
\contentsline {subsubsection}{\numberline {13.2.2}Extension: Empty word in target sentence}{22}
\contentsline {subsubsection}{\numberline {13.2.3}Extension: Refinement of alignment}{22}
\contentsline {section}{\numberline {14}Word alignment model in Neural Network}{22}
\contentsline {subsection}{\numberline {14.1}Probability approach}{22}
\contentsline {subsubsection}{\numberline {14.1.1}\cite {Tran16unsupervised}}{22}
\contentsline {paragraph}{How to apply NN ?}{23}
\contentsline {paragraph}{What is the problem of transition matrix in programming ?}{23}
\contentsline {subsection}{\numberline {14.2}Non-probability - Score approach}{24}
\contentsline {subsubsection}{\numberline {14.2.1}\cite {Yang13word}, \cite {Tamura14recurrent}}{24}
\contentsline {subsection}{\numberline {14.3}NN-based Alignment under jump-distance approach}{24}
\contentsline {section}{\numberline {15}Symmetrical alignment}{24}
\contentsline {section}{\numberline {16}Techniques in Alignment}{24}
\contentsline {subsection}{\numberline {16.1}Agreement between models}{24}
\contentsline {subsection}{\numberline {16.2}Word class for large vocabulary}{24}
\contentsline {section}{\numberline {17}Evaluation of a translation/alignment}{24}
\contentsline {section}{\numberline {18}HMM-NN Experiment}{24}
\contentsline {subsection}{\numberline {18.1}Model 1: Simple NN-based Emission}{24}
