\contentsline {part}{I\hspace {1em}Paper report}{3}
\contentsline {section}{\numberline {1}Unsupervised Neural HMMs \citep {Tran16unsupervised}}{3}
\contentsline {subsection}{\numberline {1.1}Main idea}{3}
\contentsline {subsection}{\numberline {1.2}What is their concentration ?}{3}
\contentsline {subsection}{\numberline {1.3}What is the role of HMM ?}{4}
\contentsline {subsection}{\numberline {1.4}Where is neural network ?}{4}
\contentsline {section}{\numberline {2}What does Attention in NMT pay Attention to ? \citep {Ghader2017what} }{5}
\contentsline {subsection}{\numberline {2.1}Main idea}{5}
\contentsline {subsection}{\numberline {2.2}How to compare ?}{5}
\contentsline {subsection}{\numberline {2.3}Analysis}{6}
\contentsline {section}{\numberline {3}Confidence through Attention \citep {Rikters2017confidence}}{6}
\contentsline {subsection}{\numberline {3.1}Main idea}{6}
\contentsline {subsection}{\numberline {3.2}How to calculate Confidence metric ?}{7}
\contentsline {subsection}{\numberline {3.3}Analysis}{7}
\contentsline {subsubsection}{\numberline {3.3.1}Comparison with human evaluation}{7}
\contentsline {subsubsection}{\numberline {3.3.2}Exp: Filtering Back-translated Data}{7}
\contentsline {subsubsection}{\numberline {3.3.3}Exp: Hybrid Decisions}{8}
\contentsline {section}{\numberline {4}Word Translation without Parallel data \citep {Conneau2017Word}}{8}
\contentsline {subsection}{\numberline {4.1}Main idea}{8}
\contentsline {subsection}{\numberline {4.2}How does it works ?}{8}
\contentsline {section}{\numberline {5}Word Alignment Modeling with Context Dependent Deep Neural Network \citep {Yang13word}}{9}
\contentsline {subsection}{\numberline {5.1}Main idea}{9}
\contentsline {subsection}{\numberline {5.2}How does DNN work in word alignment ?}{9}
\contentsline {subsection}{\numberline {5.3}Loss in supervised learning}{10}
\contentsline {subsection}{\numberline {5.4}Analysis}{11}
\contentsline {subsubsection}{\numberline {5.4.1}Two loss functions during training}{11}
\contentsline {subsubsection}{\numberline {5.4.2}Result}{11}
\contentsline {subsubsection}{\numberline {5.4.3}Notes}{11}
\contentsline {section}{\numberline {6}Recurrent Neural Networks for Word Alignment Model \citep {Tamura14recurrent}}{11}
\contentsline {subsection}{\numberline {6.1}Main idea}{11}
\contentsline {subsection}{\numberline {6.2}How does RNNs work in calculating alignment score ?}{11}
\contentsline {subsection}{\numberline {6.3}Loss in unsupervised learning}{12}
\contentsline {subsection}{\numberline {6.4}Agreement constraints}{12}
\contentsline {subsection}{\numberline {6.5}Analysis}{12}
\contentsline {part}{II\hspace {1em}Report}{13}
\contentsline {section}{\numberline {7}Overview about statistical alignment }{13}
\contentsline {subsection}{\numberline {7.1}Word-based alignment ? \cite {Och2003Systematic}}{13}
\contentsline {subsection}{\numberline {7.2}Hidden Markov Alignment Model}{13}
\contentsline {subsubsection}{\numberline {7.2.1}Assumption about HMM alignment probability $p(a_j | a_{j-1}, I)$ or $p(i | i', I)$}{14}
\contentsline {subsubsection}{\numberline {7.2.2}Extension: Empty word in target sentence}{14}
\contentsline {subsubsection}{\numberline {7.2.3}Extension: Refinement of alignment}{15}
\contentsline {section}{\numberline {8}Word alignment model in Neural Network}{15}
\contentsline {subsection}{\numberline {8.1}Probability approach}{15}
\contentsline {subsection}{\numberline {8.2}Non-probability - Score approach}{15}
\contentsline {section}{\numberline {9}Evaluation of a translation/alignment}{15}
