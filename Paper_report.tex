\documentclass{article}

\usepackage{graphicx} % Required for the inclusion of images
\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage[]{algorithm2e}

\setlength\parindent{0pt} % Removes all indentation from paragraphs


%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Paper Report}
\author{NGO HO Anh Khoa}

\begin{document}
\maketitle

\tableofcontents
\newpage

% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}


\part{Paper report}

%----------------------------------------------------------------------------------------
%	Unsupervised Neural HMMs
%----------------------------------------------------------------------------------------

\section{Unsupervised Neural HMMs \citep{Tran16unsupervised}}
\subsection{Main idea}
This research show how to apply unsupervised hidden Markov model in neural network approach. In fact, they would like to prove that a simple nn models trained to maximize the marginal likelihood could outperform more complicated models in unsupervised learning.

\subsection{What is their concentration ?}
\begin{itemize}
\item There are three components:
	\begin{itemize}
	\item Set of latent variables Z (Tags)
	\item Set of observed variables X (Words)
	\item Model parameters $\theta$ (Emission and transitions probability)
	\end{itemize}

\item Purpose: Find $\theta$ which maximize p(X $|\theta$)
\item How: Use Generalized EM to estimate $\theta$
	\begin{enumerate}
	\item : Maximizing $p(X)$ means
	\begin{equation}
	p(x) = \sum_{z} p(X,Z) = E_{q(Z)}[\ln p(X,Z|\theta)] + H[q(Z)] + KL(q(Z) || p(Z|X,\theta))
	\end{equation}
	\item E-step: Estimate $p(Z|X)$ based on current $\theta$
	\item M-step: Consider $q(Z) = p(Z|X)$ \\
	$KL(q(Z) || p(Z|X,\theta)) = 0$ \\
	$H[q(Z)]$ constant
	\item Maximizing $p(X)$ becomes maximizing $E_{q(Z)}[\ln p(X,Z|\theta)]$
	\end{enumerate}
\item Result: Gradient of the joint probability scaled by the posteriors
	\begin{equation}
	J(\theta) = \sum_{Z} p(Z|X) \dfrac{\partial}{\partial\theta} \ln p(X,Z| \theta)
	\label{Tran16unsupervised Gradient Joint Probability} 
	\end{equation}
\item Problem: How to calculate $p(X,Z)$ ?
\end{itemize}

\subsection{What is the role of HMM ?}
\begin{itemize}
\item Assumption:
	\begin{itemize}
	\item Every word token is generated by a latent class (Tag)
	\item The current class at time t is conditioned on the previous class at time (t - 1)
	\end{itemize}

\item Therefore, the probability of a given sequence of observation X and latent variables Z (Factorization of the joint probability):
	\begin{equation}
	p(X,Z) = \prod_{t=1}^{n + 1} p(z_{t}|z_{t-1})\prod_{t=1}^{n} p(x_{t}|z_{t})
	\label{Tran16unsupervised Probability HMM}
	\end{equation}
	
\item Result: Combine $p(X,Z)$ \eqref{Tran16unsupervised Probability HMM} and gradient $J(\theta)$ \eqref{Tran16unsupervised Gradient Joint Probability}

$J(\theta) = \sum_{Z} p(Z|X) \dfrac{\partial}{\partial\theta} \ln p(X,Z| \theta)$ \\

$ = \sum_{Z} p(Z|X) \dfrac{\partial}{\partial\theta} \ln [\prod_{t=1}^{n + 1} p(z_{t}|z_{t-1},\theta)\prod_{t=1}^{n} p(x_{t}|z_{t},\theta) ]$ \\

$ = \sum_{Z} p(Z|X) \dfrac{\partial}{\partial\theta}  [\sum_{t=1}^{n + 1} \ln p(z_{t}|z_{t-1},\theta) + \sum_{t=1}^{n} \ln p(x_{t}|z_{t},\theta) ]$

$ = \sum_{t}\sum_{z_{t}} p(z_{t}, z_{t-1}|X) \dfrac{\partial}{\partial\theta} \ln p(z_{t}|z_{t-1},\theta) + p(z_{t}|X) \dfrac{\partial}{\partial\theta} \ln p(x_{t}|z_{t},\theta) $

\begin{equation}
	J(\theta) = \sum_{t}\sum_{z_{t}} p(z_{t}, z_{t-1}|X) \dfrac{\partial}{\partial\theta} \ln p(z_{t}|z_{t-1},\theta) + p(z_{t}|X) \dfrac{\partial}{\partial\theta} \ln p(x_{t}|z_{t},\theta)
\end{equation}

\item Problem:
	\begin{itemize}
	\item How to calculate $p(z_{t}, z_{t-1}|X)$ and $p(z_{t}|X)$ ? They propose Baum-Welch
	\item How to calculate $p(z_{t}|z_{t-1},\theta)$ and $p(x_{t}|z_{t},\theta)$ ? They propose Neural Networks
	\end{itemize}
\end{itemize}

\subsection{Where is neural network ?}
\begin{itemize}
\item Input: A sentence $X = x_{1},..., x_{t},... x_{L_{x}}$, a set of vocabulary $W = w_{1},..., w_{i},... w_{L_{W}}$ a set of tags $Z = z_{1},..., z_{j},... z_{L_{z}}$
\item Output: $p(z_{t}|z_{t-1},\theta)$ and $p(x_{t}|z_{t},\theta)$ at each time t.

\item How:
	\begin{enumerate}
	\item Embedding X and Z by $\theta$: Vector embedding of W $v_{W}$ (Using CNN - Convolution for Morphology) and vector embedding of Z $v_{Z}$ (Simple feed-forward nn having a lookup table following by a non-linear activation ReLU). $v_{W}$ and $v_{Z}$ have the same dimension.
	\item Calculate $p(x_{t}|z_{t},\theta)$ (Emission matrix): Probability of a word $w_{i}$ in Vocabulary is generated by a tag $z_{j}$ (Do not care about time t).
	\begin{equation}
	p(w_{i}|z_{j}) = \frac{exp(v_{z_{j}}^{T}*v_{w_{i}} + b_{i})}{\sum_{w \in W} exp(v_{z_{j}}^{T}*v_{w} + b)}
	\end{equation}

	\item Calculate $p(z_{t}|z_{t-1},\theta)$ (Transition matrix): Probability of a tag $z_{j}$ at time t is generated by a tag $z_{j'}$ at time (t - 1).\\
	Input: Vector of word w at $x_{t}$ noted $v_{x_{t}}$. It is query embedding (Using LSTMs) \\
	Result: Matrix of $L_{z}*L_{z}$ noted T. It means all transition probabilities of each tag at (t - 1) to all tags at time t.\\
	\begin{equation}
	T = U^{T}*v_{x_{t}} + b
	\end{equation}
	\end{enumerate}
\end{itemize}

%----------------------------------------------------------------------------------------
%	What does Attention in NMT pay Attention to ?
%----------------------------------------------------------------------------------------

\section{What does Attention in NMT pay Attention to ? \citep{Ghader2017what} }
\subsection{Main idea}
This research compares between Attention Models (Non-recurrent attention model/ Global attention and Recurrent attention/ Input-feeding model) and known Word Alignment.
The result is that their differences depends on the word type being generated.
\subsection{How to compare ?}
Higher consistency between Attention and Alignment leading to better translation.

\begin{itemize}
\item \textbf{Spearman's rank correlation between attention quality} (Attention loss compared known human alignment) \textbf{and translation quality} (Word prediction loss).
Higher correlation means a closer relationship between translation quality and consistency of attention versus alignment.
	\begin{itemize}
	\item Attention loss
		\begin{equation}
		L_{At}(outToken) = - \sum_{inToken} Al(in, out) * log(At(in, out))
		\end{equation}
		\begin{itemize}
		\item Al(in,out): Weight of alignment link between input token and output token
		\item At(in,out): Weight of attention between input token and output token
		\end{itemize}

	\item Word prediction loss: Softmax()
	\end{itemize}
	
\item Attention concentration: Entropy of attention distribution (Soft-hard attention problem)
	\begin{equation}
	E_{At}(outToken) = - \sum_{in} At(in, out) * log (At(in, out))
	\end{equation}
	\begin{itemize}
	\item At(in,out): Weight of attention between input token and output token
	\end{itemize}
\end{itemize}
\subsection{Analysis}
The analysis is based on POS tags experiments.
\begin{itemize}
\item Impact of Attention between Non-recurrent (NR) and Recurrent attention (IF): IF has lower AER (Hard attention) and Attention loss (Soft attention).
\item Translation quality: Consistency between attention and word alignment depends on POS tags. For example,
	\begin{itemize}
	\item There is a higher consistency in the case of nouns. However, attention captures other information in the case of verbs.
	\item Translation quality of Verbs is better than Nouns. It means attention does not follow alignment for translating Verbs.
	\end{itemize}
\item Attention concentration: Review the case of Verbs and Nouns.
	\begin{itemize}
	\item Nouns have a lower attention entropy (Higher concentration), lower attention loss (Closer to Alignment), which is that attention entropy can be used as a measure of closeness of attention to alignment in the case of nouns
	\item Verbs have a lower correlation between attention entropy and word prediction loss, which means that attention concentration is not necessary for translating verbs.
	\end{itemize}
\item Attention distribution: It shows how a POS tag of target sentence depends on other POS tags of source sentence.
\end{itemize}

%----------------------------------------------------------------------------------------
%	What does Attention in NMT pay Attention to ?
%----------------------------------------------------------------------------------------

\section{Confidence through Attention \citep{Rikters2017confidence}}

\subsection{Main idea}
\textbf{Auto-evaluation metric without reference.}
This research is that attention distribution becomes a confidence metric (Translation quality and Decoder confidence)
\begin{itemize}
\item Filtering out bad translation from a large back-translated corpus (Provide a better parallel corpus)
\item Selecting the best translation in a hybrid setup of 2 translation systems
\end{itemize}
The result is that this metric could be consider as an human judgment (Not so true!, just about 50\%) and leads to BLEU score improvement (in some cases).

\subsection{How to calculate Confidence metric ?}
Penalty measures: Coverage deviation and Absentmindedness
\begin{itemize}
\item Coverage Deviation Penalty: Lacking attention and Too much attention per input token, which mean penalizing the sum of attention per input token for going to far from 1.0 (Why 1.0: Replaced by token's expected fertility)

\begin{equation}
CDP = -\frac{1}{inSentLen}\sum_{inToken} log(1 + (1 - \sum_{outToken} \alpha_{out-inToken})^2 )
\end{equation}

\item Absentmindedness Penalty (Entropy): The attention of confident output tokens should concentrate on a small number of input tokens and vice versa (Assumption).
\begin{equation}
AP_{out} = -\frac{1}{inSentLen}\sum_{inToken} \sum_{outToken} \alpha_{out-inToken} * log (\alpha_{out-inToken})
\end{equation}

\item Combination:
\begin{equation}
Confidence = CDP + AP_{out} + AP_{in}
\end{equation}
\end{itemize}

\subsection{Analysis}
\subsubsection{Comparison with human evaluation}
They use Kendall rank correlation coefficient for looking at the pairs where human scores differ. They recognize that their metric over-penalizes the translations which do not follow the source word-by-word.
\subsubsection{Exp: Filtering Back-translated Data}
They compare their confidence metric with language model method in filtering the best translated sentences. Both methods have the similar levels of overlapping the human evaluation. One point should be considered is that their metric does not require any additional model (LM).

For BLEU score, their method show a better performance on some cases, which is in general insignificant. 

\subsubsection{Exp: Hybrid Decisions}
The difference between two baseline systems influences on the final BLEU score. A small difference leads to the small improvements, a large difference causes a score drop. It is well-reported that hybrid selection overlaps about 50\% human selection.

%----------------------------------------------------------------------------------------
%	Word Translation without Parallel data
%----------------------------------------------------------------------------------------

\section{Word Translation without Parallel data \citep{Conneau2017Word}}

\subsection{Main idea}
The research is about \textbf{building a bilingual dictionary without parallel data} by aligning monolingual word embedding spaces in a unsupervised way (GAN) and proposing a similarity metric CSLS.

Its results show a strong performance of using Procrustes-CSLS. GAN in this case is a step necessary to overcome unsupervised learning. 

\subsection{How does it works ?}
Input: Two large monolingual corpora.\\
Output: Linear mapping W between the source and target space.
Two steps of training:
\begin{enumerate}
\item Training GAN:
	\begin{itemize}
	\item A discriminator distinguishes between n mapped source embeddings and m mapped target embeddings.\\
	Objective function:
	\begin{equation}
	Loss_{D}(\theta_{D}| W) = - \frac{1}{n} \sum_{i=1}^{n} \log P_{\theta_{D}}(source=true|Wx_{i}) - \frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_{D}}(source=false|y_{i})
	\end{equation}
	
	\item A generator creates these embeddings to fool discriminator.\\
	Objective function:
	\begin{equation}
	Loss_{W}(W|\theta_{D}) = - \frac{1}{n} \sum_{i=1}^{n} \log P_{\theta_{D}}(source=false|Wx_{i}) - \frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_{D}}(source=true|y_{i})
	\end{equation}
	
	\item Update parameter:\\
	Orthogonality advantages: Reservation of monolingual quality of the embeddings (Dot product of vectors or distances, rotation $\rightarrow$ An isometry of the Euclidean space); Stable training)
	\begin{equation}
	W \leftarrow (1 + \beta)W - \beta(WW^{T})W
	\end{equation}
	
	
	\end{itemize}
\item Refinement procedure (Solution for rare words that GAN does not well solve) repeats until reaching stopping condition.
	\begin{enumerate}
	\item Extracting a synthetic high-quality dictionary (Most frequent words) evaluated by CSLS
	\item Applying Procrustes solution for generating more accurate dictionary.
	\end{enumerate}
\item Stopping condition/Best hyper-parameters selection - Validation step: Similarity measure CSLS between mapped source and target words.\\
	Why: It is based on K-NN and overcomes a problem of two spaces and "Hubs and Anti-hubs" (Some points are highly near many other points while there are some points are not nearest any point in high-dimensional spaces).\\
	They proposed a bipartite neighborhood graph. Each word of a language is connected to K words of an other language.
	\begin{equation}
	CSLS(Wx_{s}, y_{t}) = 2 \cos(Wx_{s}, y_{t}) - r_{T}(Wx_{s}) - r_{S}(y_{t})
	\end{equation}
		\begin{itemize}
		\item $r_{T}(Wx_{s}) = \frac{1}{K} \sum_{y_{t} \in N_{T}(Wx_{s})} \cos(Wx_{s}, y_{t})$\\
		Mean similarity of a source embedding $x_{s}$ to its target neighbors.
		\item $r_{S}(y_{t}) = \frac{1}{K} \sum_{Wx_{s} \in N_{S}(y_{t})} \cos(y_{t}, Wx_{s})$\\
		Mean similarity of a source embedding $x_{s}$ to its target neighbors.
		\item $N_{T}(Wx_{s})$: Neighbor of a source word $Wx_{s}$ in target word space.
		\end{itemize}

\end{enumerate}

%----------------------------------------------------------------------------------------
%	Alignment by agreement
%----------------------------------------------------------------------------------------
\section{Alignment by Agreement \citep{Liang2006Alignment}}
\subsection{Main idea}
This approach is similar to ensemble method. In this unsupervised approach, the two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. The new point is encouraging the agreement between these models during training.

In fact, intersecting the alignment prediction of two asymmetric models reduces AER. An assumption is that this intersection (Agreement) could reduce the loss of each asymmetric model.

Moreover, we could see the transformation from asymmetric models to symmetric model.

\subsection{How to train these two model ?}
\begin{itemize}
\item Two models are:
\begin{equation}
\begin{split}
&p_1(x,z; \theta_1) = p(e)p(a,f| e; \theta_1)  \\
&p_2(x,z; \theta_2) = p(f)p(a,e| f; \theta_2)
\end{split}
\end{equation}
where \\
$x = (e,f)$ is an input sentence pair, \\
$z = {z_{i,j} \in \{0,1\} : 1 \leq i \leq I, 1 \leq j \leq J}$ is a set of indicator variables for each potential alignment edge.

\item The data likelihood:
\begin{equation}
\prod_x p_k(x; \theta_k) = \prod_x \sum_z p_k(x,z, \theta_k)
\end{equation}
where $k \in {1, 2}$ is models.

\item \textbf{Joint objective function} is maximizing the data likelihood:
\begin{equation}
\max_{\theta_1, \theta_2} \sum_x [log p_1(x;\theta_1) + log p_2(x;\theta_2)] \\
\end{equation}
Because of the probability that two models agree the alignments on an example $x$ being
\begin{equation}
\sum_z p_1(z
|x;\theta_1) p_2(z|x; \theta_2)
\end{equation}

\item Therefore,
\begin{equation}
\max_{\theta_1, \theta_2} \sum_x [log p_1(x;\theta_1) + log p_2(x;\theta_2) + log\sum_z p_1(z
|x;\theta_1) p_2(z|x; \theta_2)]
\end{equation}
\end{itemize}

\subsubsection{Optimization via EM}
From the standard EM,
\begin{equation}
\begin{split}
& E: q(z;x) := p(z|x ;\theta) \\
& M: \theta^{'} := argmax_{\theta} \sum_{x,z} q(z;x) \quad log \,  p(x,z;\theta)
\end{split}
\end{equation}

becomes

\begin{equation}
\begin{split}
& E: q(z;x) := \frac{1}{Z_x} p_1(z|x ;\theta_1) p_2(z|x ;\theta_2) \\
& M: \theta^{'} := argmax_{\theta} \sum_{x,z} q(z;x) \, log \,  p_1(x,z;\theta_1) +  \sum_{x,z} q(z;x) \, log \,  p_2(x,z;\theta_2)
\end{split}
\end{equation}

where $Z_x$ is a normalization constant. It is sum of the product of two model posteriors over the set of possible $z$ with nonzero probability under both models.\\

In E-step,
\begin{equation}
 E: q(z;x) := \frac{1}{Z_x} p_1(z|x ;\theta_1) p_2(z|x ;\theta_2)
\end{equation}

becomes

\begin{equation}
q(z;x) := \prod_{i,j} p_1(z_{ij} | x; \theta_1) \, p_2(z_{ij} | x; \theta_2)
\end{equation}
where each $p_k(z_{i,j}|x; \theta_k)$ is the posterior marginal probability of the (i,j) edge.\\

This posterior $q(z;x)$ could show the disagreement because this product $p_1(z|x ;\theta_1) p_2(z|x ;\theta_2)$ is small.

\subsection{Analysis}
This agreement approach does not guarantee to increase the joint objective. The experiment shows that it could provide an effective method of achieving model agreement and a significant accuracy gains over independent training.

%----------------------------------------------------------------------------------------
%	Word Alignment Modeling with Context Dependent Deep Neural Network
%----------------------------------------------------------------------------------------

\section{Word Alignment Modeling with Context Dependent Deep Neural Network \citep{Yang13word}}

\subsection{Main idea}
The research describes how to use Context Dependent Deep NN in HMM-based word alignment model. This means that bilingual word embedding could capture not only lexical translation and context from surrounding words.
An example is translating a rare word by using its surrounding words.\\
It is noted that this version uses a smaller number of parameters than the classic HMM model.

\subsection{How does DNN work in word alignment ?}
The role of DNN in this paper is learning automatically feature from raw text.
They start from the factorization of the joint probability \cite{Vogel1996HMM}.
\begin{equation}
p(a, e| f) = \prod_{i = 1}^{|e|} P_{lex}(e_{i}|f_{a_{i}}) P_{d}(a_{i}|a_{i-1})
\end{equation}
	\begin{itemize}
	\item Given a sentence pair (e,f) 
	\item $P_{lex}$: Lexical translation probability, emission probability
	\item $P_{d}$: HMM alignment probability \cite{Vogel1996HMM}, transition probability. The paper has a different notation: $P_{d}(a_{i} - a_{i-1})$ and it is called Jump distance distortion probability (Why: The alignment probability depends only on the jump width $(a_{i} - a_{i-1})$ \cite{Vogel1996HMM}).
	\end{itemize}

They propose using a score instead of these probabilities because they would like to avoid the softmax normalization step of a large vocabulary. The formula above becomes:

\begin{equation}
s_{NN}(a|e, f) = \prod_{i = 1}^{|e|} t_{lex}(e_{i},f_{a_{i}}|e,f) t_{d}(a_{i},a_{i-1}|e,f)
\label{Yang13word NN Alignment Score}
\end{equation}

	\begin{itemize}
	\item $s_{NN}(a|e, f)$ is the score of an alignment based on the source and target sentence.
	\item $f_{a_{i}}$ represents not only this word but the context around this word. Surrounding words of both source and target word are input of DNN, which is handling the context of both sides. This could reduce the explosion of parameter number. \\ In this case, they use fixed length windows surrounding both $e_{i}$ and $f_{j}$. 
	\item $t_{lex}$: Lexical translation score.
	\begin{equation}
	t_{lex}(e_{i},f_{j}|e,f) = functions_{NN}(window(e_{i}),window(f_{j}) )
	\label{Yang13word Lexical Translation Score}
	\end{equation}
	\item $t_{d}$: Distortion score.
	\begin{equation}
	t_{d}(a_{i}, a_{i-1}|e,f) = t_{d}(a_{i} - a_{i-1}|window(f_{a_{i-1}})) = functions_{NN}(window(f_{a_{i-1}}))
	\end{equation}
	They recognize that this lexicalized distortion does not produce a better alignment. They reverse to the simple version.
	\begin{equation}
	t_{d}(a_{i}, a_{i-1}|e,f) = t_{d}(a_{i} - a_{i-1})
	\end{equation}
	\end{itemize}

\subsection{Loss in supervised learning}
\begin{equation}
loss(\theta) = \sum_{every(\textbf{e},\textbf{f})} \max(0 , 1 - s_{\theta}(a^+ |\textbf{e},\textbf{f}) + s_{\theta}(a^- |\textbf{e},\textbf{f}) )
\label{Yang13word Alignment Loss}
\end{equation}
where
\begin{itemize}
\item $a^+$: Gold alignment path
\item $a^-$: Highest scoring incorrect alignment path under $\theta$
\item $s_\theta$: Score for alignment path defined in \eqref{Yang13word NN Alignment Score}
\end{itemize}

\subsection{Analysis}
\subsubsection{Two loss functions during training}
They pre-train word embedding with monolingual data.
They train firstly NN with the loss from lexical translation score and then the loss \eqref{Yang13word Alignment Loss} because they recognize that using only the loss \eqref{Yang13word Alignment Loss} is not efficient.

The loss of lexical translation score:
\begin{equation}
loss(\theta) = \sum_{every(\textbf{e},\textbf{f})} \max(0 , 1 - t_{lex, \theta}((e,f)^+ | \textbf{e},\textbf{f}) + t_{lex, \theta}((e,f)^- | \textbf{e},\textbf{f}) )
\end{equation}
where
\begin{itemize}
\item $(e,f)^+$: Correct word pair
\item $(e,f)^-$: Incorrect word pair
\item $t_{lex}$: Score of \eqref{Yang13word Lexical Translation Score}
\end{itemize}

\subsubsection{Result}
Baselines are HMM and IBM4. IBM4+NN takes the first place, followed by IBM4 and then HMM+NN.

\subsubsection{Notes}
They conclude also that the size of window influence on the accuracy of translation score.
The large number of hidden layer does not return any improvement.

%----------------------------------------------------------------------------------------
%	Recurrent Neural Networks for Word Alignment Model
%----------------------------------------------------------------------------------------
\section{Recurrent Neural Networks for Word Alignment Model \citep{Tamura14recurrent}}

\subsection{Main idea}
This research is mainly based on \citep{Yang13word}. The difference is that the score of alignment is calculated in recurrent approach RNNs, which means that $a_j$ depends on all previous position $a_{0...j-1}$.
The score of an alignment in \citep{Yang13word} has two components: One is for lexical translation and the other is for alignment. This research proposes a single score.

\subsection{How does RNNs work in calculating alignment score ?}

The equation of \citep{Yang13word} (Reverse the source and the target sentence)
\begin{equation}
s_{NN}(a_1^ J|e_1^ I, f_1^ J) = \prod_{j = 1}^{J} t_{d}(a_{j},a_{j-1}|window(e_{a_{j-1}})) *t_{lex}(f_{j},e_{a_{j}}|window(e_{a_{j}}), window(f_{j}))
\end{equation}

It is modified for RNNs:

\begin{equation}
s_{NN}(a_1^ J|e_1^ I, f_1^ J) = \prod_{j = 1}^{J} t_{RNN}(a_{j}|a_1^{j-1},e_{a_{j}},f_j  )
\end{equation}

\subsection{Loss in unsupervised learning} 
\begin{equation}
loss(\theta) = max(0 , 1 - \sum_{(\textbf{f}^+,\textbf{e}^+) \in T} E_{\Phi} [s_{\theta} (\textbf{a}|\textbf{f}^+, \textbf{e}^+)]  + \sum_{(\textbf{f}^+,\textbf{e}^-) \in \Omega} E_{\Phi} [s_{\theta} (\textbf{a}|\textbf{f}^+, \textbf{e}^-)] )
\label{Tamura14recurrent Loss Unsupervised Learning}
\end{equation}
where
\begin{itemize}
\item T: Training data (Bilingual sentences) as Observed data
\item $\Omega$ : Full translation search space as neighborhood of observe data, .
\item $\Phi$ : Set of all possible alignments given (\textbf{e},\textbf{f})
\item $E_{\Phi} [s_{\theta}]$ : Expected value of the scores $s_\theta$ on $\Phi$
\item $e^+$ : A target language sentence in T
\item $e^-$ : A pseudo-target language sentence
\item $\sum_{(\textbf{f}^+,\textbf{e}^+) \in T} E_{\Phi} [s_{\theta} (\textbf{a}|\textbf{f}^+, \textbf{e}^+)]$ : Expectation term for observed data
\item$ \sum_{(\textbf{f}^+,\textbf{e}^-) \in \Omega} E_{\Phi} [s_{\theta} (\textbf{a}|\textbf{f}^+, \textbf{e}^-)] )$ : Expectation term for neighborhood
\end{itemize}

They would like to reduce the computation, they randomly select N pseudo-target language sentences for each $f^+$. \eqref{Tamura14recurrent Loss Unsupervised Learning} becomes:
\begin{equation}
loss(\theta) = \sum_{\textbf{f}^+ \in T} max(0 , 1 -  E_{GEN} [s_{\theta} (\textbf{a}|\textbf{f}^+, \textbf{e}^+)]  + \frac{1}{N} \sum_{\textbf{e}^-} E_{GEN} [s_{\theta} (\textbf{a}|\textbf{f}^+, \textbf{e}^-)] )
\end{equation}
where
\begin{itemize}
\item $\textbf{e}^-$ : a pseudo-target language sentence with the same length $|\textbf{e}^-| = |\textbf{e}^+|$
\item GEN: A subset of all possible alignment generated by beam search
\end{itemize}

\subsection{Agreement constraints}
\marginpar{See \citep{Liang2006Alignment}}

Why: HMM-based model is asymmetric. It is demonstrated that encouraging directional models to agree improves alignment performance.
\\
How: They propose training two directional models including alignment F$\rightarrow$E and E$\rightarrow$F. Each model has a different modified loss function 

\begin{equation}
loss_{\theta_{F \rightarrow E}} = loss(\theta_{F \rightarrow E})  + \alpha \parallel \theta_{E \rightarrow F} - \theta_{F \rightarrow E} \parallel
\end{equation}

\begin{equation}
loss_{\theta_{E \rightarrow F}} = loss(\theta_{E \rightarrow F})  + \alpha \parallel \theta_{F \rightarrow E} - \theta_{E \rightarrow F} \parallel
\end{equation}

where
\begin{itemize}
\item $\theta_{E \rightarrow F}$ : Weight of layers in model $E \rightarrow F$
\item $\alpha$: Weight parameter of the agreement constraint $\parallel \theta_{E \rightarrow F} - \theta_{F \rightarrow E} \parallel$
\item $\parallel \theta \parallel$: Norm of $\theta$
\item $loss(\theta_{F \rightarrow E})$: Loss from \eqref{Tamura14recurrent Loss Unsupervised Learning}
\end{itemize}


\subsection{Analysis} 
Baselines are IBM4, FFNN \citep{Yang13word}. They do the experiments of word alignment and machine translation in both unsupervised/supervised learning.

\subsubsection{Result}
RNNs captures alignment paths based on long alignment history. This can be viewed as phrase-level alignment, which is more effective in non-similar-order-language (Japanese-English) than French-English.\\

RNNs could help to reduce the size of training data. They show the similar results between RNNs-Small data and IBM4-Large data.\\

They highlight the role of agreement constraints in alignment performance improvement.\\



%----------------------------------------------------------------------------------------
%	Hierarchical Multiscale Recurrent Neural Networks
%----------------------------------------------------------------------------------------

\section{Hierarchical Multiscale Recurrent Neural Networks \citep{Chung2016Hierarchical}}

\subsection{Main idea}
Hierarchical Multi-scale RNNs learns the latent hierarchical structure of a sequence and its temporal representation with non-fixed timescales (Without explicit boundary information).

For example, this model consider a sentence as a list of characters, not only words. It is trained to know autonomously the beginning and the end of a word. For each layer, it could group the units.

The key idea is an update mechanism with a binary boundary detector (Gate determining the boundary of an object).\\

The advantages of "Multi-scale" over standard RNNs:
\begin{itemize}
\item Updating the high-level layers is less frequently $\rightarrow$ Computational efficiency and Vanishing gradient problem. Solving vanishing gradient problem means that this model could delivering efficiently long-term dependencies.
\item Flexible resource allocation. The model could modify the number of hidden units for an information. For example, the model could use more units to the higher layers (Long-term dependency) and less units to the lower layers (Short-term dependency). This means that it could decide to keep the long-term or short-term information.

\item A good generalization performance (Dropout approach)
\item The internal process of RNN becomes more interpretable. We could count the number of operation execution time ( Update, Flush and Copy operation)
\end{itemize}

The problem of the previous kind of RNNs:
\begin{itemize}
\item LSTMs: LSTMs has also an update mechanism with forget and update gates, which means that it could operate with different timescales. However, these timescales are not organized hierarchically. \\
Problem:
	\begin{itemize}
	 \item In practice, after a few hundred time steps, long-term information is gradually diluted at every time step.
	 \item LSTMs is computationally expensive (Update at every time step for each unit)
 	\end{itemize} 
 	
\item Clockwork RNNs and its soft timescale approach: The hidden units are grouped into several modules. Different timescales are assigned to these modules (i.e: A module $i$ is updated at every $2^{i - 1}$ -th time step).\\
Problem:
	\begin{itemize}
	\item It is not easy to find a good timescales because timescale depends on data.
	\item It is unclear how to deploy more layers than the number of boundary levels that is explicitly observed in the data.
	\end{itemize}
	
\item Hierarchical RNNs with Known explicit hierarchical boundary structure (Not multi-scale).\\ Problem: It is the same with Clockwork RNNs that boundary information is not explicit and expensive to be discovered.

\item CNNs with 1-D kernels: It could provide also high-level representation of sequence.\\
Problem: We need to observe a full sequence with a reduced length.

\item Zoneout, a regularization technique similar to dropout.\\
Problem: Its strength is still an hyper-parameter.

\end{itemize}

\subsection{Update mechanism}
They describe Hierarchical Multi-scale RNN (HM-RNN) based on LSTM update rule. It is better to see the graph in paper.\\
\marginpar{Why it is not based on RNN ?}

At each time step $t$, for each layer $l$, there are:
\begin{equation}
h_t^l,\, c_t^l,\, z_t^l = f^l (c_{t-1}^l, h_{t-1}^l, h_t^{l-1}, h_{t-1}^{l+1}, z_{t-1}^l, z_t^{l-1})
\end{equation}

The function includes Update, Copy and Flush operation depending on binary detector - boundary states $z_{t-1}^l$ (Previous time step) and $z_t^{l-1}$ (Below layer).
The difference between three operations are the use of context $c$ which keeps the summary information of the previous units.
\begin{itemize}
\item \textbf{Flush}: It shows the beginning of a segment or "word", the boundary is found previously.
\\ The unit $h_t^l$ does not get information from same-level context $c_{t-1}^l$ (Eject) but higher-level unit $h_{t-1}^{l+1}$ (This is the connection between two words, it is re-installed with more long-term information from previous words). 

\item  \textbf{Copy}: It shows not-end state of a "word", none of boundary is found.
\\ the unit  $h_t^l$ only copies from previous unit $h_{t-1}^l$, its context copies from $c_{t-1}^l$ (It reduces the number of operation execution, leading to computational efficiency).

\item \textbf{Update}: It is similar to update operation of LSTM (Forget, input and output gate) but it is executed sparsely. It is the end of a "word", the boundary is found in the unit below.
\\ The unit $h_t^l$ gets also information from the same-level context $c_{t-1}^l$ and information from the lower-level unit $h_t^{l-1}$, but not from the higher-level unit $h_{t-1}^{l+1}$ (It does not need information from previous words).
\end{itemize}

\subsection{Analysis}
The result for character-level language model does not show clearly a good performance.

%----------------------------------------------------------------------------------------
%	Hybrid Neural Network alignment and Lexicon Model in direct HMM for Statistical Machine Translation
%----------------------------------------------------------------------------------------

\section{Hybrid Neural Network alignment and Lexicon Model in direct HMM for Statistical Machine Translation}



%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	Report
%----------------------------------------------------------------------------------------
\pagebreak 
\part{Report}

\section{Overview about statistical alignment }

\subsection{Word-based alignment ? \cite{Och2003Systematic}}
Input:
\begin{itemize}
\item A source sentence: $f_1^J = f_1,...,f_j,...f_J$
\item A target sentence: $e_1^I = f_1,...,f_i,...f_I$
\end{itemize}
Output: Alignment map $j \rightarrow i = a_j$
\begin{itemize}
\item An alignment: $a_1^J = a_1,...,a_j,...a_J$
\end{itemize}


Our work is modelling the relationship between a source sentence and a target sentence. \\
We start from the view of translation model, which is finding this best translation $e_1^I$ for a source sentence $f_1^J$.
\begin{equation}
e_1^I = argmax_{e_1^I} p(e_1^I|f_1^J) = argmax_{e_1^I} \frac{p(f_1^J|e_1^I)p(e_1^I)}{p(f_1^J)}
\end{equation}

In this case, the translation direction is changed from $p(e_1^I|f_1^J)$ to $p(f_1^J|e_1^I)$. This should be noted while reading alignment papers. \\

From now, the work of translation is modeling $p(f_1^J|e_1^I)$. Our work is more complicated by adding an alignment component $a_1^J$ which maps from a source position j to a target position $a_j$ (It could be i, but there is also empty words).

\begin{equation}
\end{equation}

With the model parameters $\theta$, our main problem becomes $p_{\theta}(f_1^J, a_1^J|e_1^I)$

\begin{equation}
p_{\theta}(f_1^J|e_1^I) = \sum_{a_1^J} p_{\theta}(f_1^J, a_1^J|e_1^I)
\label{Overview about Statistical Alignment p(f,e)}
\end{equation}


\subsection{Hidden Markov Alignment Model \cite{Och2003Systematic}}
The alignment model is re-structured:
\begin{equation}
p(f_1^J, a_1^J|e_1^I) = p(J|e_1^I) \prod_{j=1}^J p(f_j, a_j | f_1^{j-1}, a_1^{j-1}, e_1^I)
\end{equation}
\begin{equation}
= p(J|e_1^I) \prod_{j=1}^J p( f_j | f_1^{j-1}, a_j, e_1^I) * p( a_j | f_1^{j-1}, a_1^{j-1}, e_1^I)
\end{equation}

For this new structure, there are three different probabilities:
\begin{itemize}
\item $p(J|e_1^I)$ : Length probability
\item $p( f_j | f_1^{j-1}, a_j, e_1^I)$ : Lexicon  probability
\item $p( a_j | f_1^{j-1}, a_1^{j-1}, e_1^I)$ : Alignment probability
\end{itemize}

We need some assumptions to put this model into HMM:
\begin{itemize}
\item $p( f_j | f_1^{j-1}, a_j, e_1^I) \rightarrow p(f_j | e_{a_j}) $ : The lexicon probability depends only on the word at position $a_j$
\item $p( a_j | f_1^{j-1}, a_1^{j-1}, e_1^I) \rightarrow p(a_j | a_{j-1}, I) $ : The alignment $a_j$ depends on $a_{j - 1}$  (First-order dependence)
\item $p(J|e_1^I) \rightarrow p(J|I)$ : Simplify this probability (Not because of HMM, it's just simplification)
\end{itemize}

Therefore, $p(f_1^J, a_1^J|e_1^I)$ is decomposed under these assumptions as follows:

\begin{equation}
p(f_1^J, a_1^J|e_1^I) = p(J|I) \prod_{j=1}^J p(f_j | e_{a_j}) * p(a_j | a_{j-1}, I)
\label{Och2003Systematic Alignment Model Equation}
\end{equation}

From this formula, we need to calculate these two components:
\begin{itemize}
\item $ p(f | e) $ : Translation probability
\item $p(a_j | a_{j-1}, I)$ or $p(i | i', I)$ : HMM alignment probability
\end{itemize}

\subsubsection{Assumption about HMM alignment probability $p(a_j | a_{j-1}, I)$ or $p(i | i', I)$}
Alignment probability depends on the difference in the alignment positions rather than on the absolute position \cite{Vogel1996HMM}.

\begin{equation}
p(i | i', I) = \frac{c(i - i')}{\sum_{i''=1}^I c(i'' - i')}
\end{equation}
where $c(i - i')$ is non-negative.

\subsubsection{Extension: Empty word in target sentence}
An source word could give an empty word, which is that each target word has an extra empty word. This leads to the length of target sentence being $e_{1}^{2I}$ and the empty word zone being $e_{I+1}^{2I}$. The word $e_{i}$ has an empty word $e_{i+I}$.

They enforce the constraints:
\begin{equation}
p(i + I | i', I) = p_{0} \delta(i, i')
\end{equation}
\begin{equation}
p(i + I | i' + I, I) = p_{0} \delta(i, i')
\end{equation}
\begin{equation}
p(i | i' + I, I) = p(i | i', I )
\end{equation}

where $p_{0}$ is the probability of a transition to the empty word.

\subsubsection{Extension: Refinement of alignment}

\begin{itemize}
\item Purpose: Add more assumptions that $p(a_j | a_{j-1}, I)$ depends on $e_{a_{j -1}}$ or $f_j$.
\item Problem: In the case of large corpora or large size of vocabulary, it leads to a large alignment parameters.
\item Solution: They use classes G which are the mappings of words to classes.
\end{itemize}

\section{Word alignment model in Neural Network}
We start from this formula \eqref{Overview about Statistical Alignment p(f,e)}:

\begin{equation}
p_{\theta}(f_1^J|e_1^I) = \sum_{a_1^J} p_{\theta}(f_1^J, a_1^J|e_1^I)
\label{Word Alignment Model Neural Network p(f,e)}
\end{equation}

\subsection{Probability approach}
\subsubsection{\cite{Tran16unsupervised}}
Unsupervised neural HMM of \cite{Tran16unsupervised} shows how to apply NN in HMM. In general, we try to calculate the probability by using two different neural networks.The significant point of NN is embedding context.

\begin{enumerate}
\item They use Generalized EM to estimate $\theta$, \eqref{Word Alignment Model Neural Network p(f,e)} becomes:\\

$p_{\theta}(f_1^J|e_1^I) = \sum_{a_1^J} p_{\theta}(f_1^J, a_1^J|e_1^I)$

$ = E_{q(a_1^J)} [ln p_{\theta}(f_1^J, a_1^J|e_1^I) ] + H[q(a_1^J)] + KL(q(a_1^J) \| p_{\theta}(a_1^J|f_1^J,e_1^I) ) $

\item Updating $\theta$ requires only maximizing $E_{q(a_1^J)} [ln p_{\theta}(f_1^J, a_1^J|e_1^I) ]$. \\ The gradient is: \\
\marginpar{Why?: See \citep{Tran16unsupervised}}

$ J(\theta) = \sum_{a_1^J} \quad q(a_1^J)\quad \bigtriangledown log \, p_{\theta}(f_1^J, a_1^J|e_1^I)  $

\item In the step E, we model $q(a_1^J) = p_{\theta'}(a_1^J|f_1^J,e_1^I)$.
Therefore,\\

$ J(\theta) = \sum_{a_1^J} \quad p_{\theta'}(a_1^J|f_1^J,e_1^I) \quad \bigtriangledown log \, p_{\theta}(f_1^J, a_1^J|e_1^I)  $

\item From \eqref{Och2003Systematic Alignment Model Equation}, we have \\
$p(f_1^J, a_1^J|e_1^I) = \prod_{j=1}^J p(f_j | e_{a_j}) * p(a_j | a_{j-1})$  with the assumptions about I, J.
Therefore, \\

$ J(\theta) = \sum_{a_1^J} \quad p_{\theta'}(a_1^J|f_1^J,e_1^I) \quad \bigtriangledown log \, p_{\theta}(f_1^J, a_1^J|e_1^I)  $

$ = \sum_{a_1^J} \quad p_{\theta'}(a_1^J|f_1^J,e_1^I) \quad \bigtriangledown log \, [\prod_{j=1}^J p_{\theta}(f_j | e_{a_j}) * p_{\theta}(a_j | a_{j-1})  ] $

$ = \sum_{a_1^J} \quad p_{\theta'}(a_1^J|f_1^J,e_1^I) \quad \bigtriangledown [\sum_{j=1}^J log \, p_{\theta}(f_j | e_{a_j}) + log \, p_{\theta}(a_j | a_{j-1})  ] $

$ = \sum_{j=1}^J \sum_{a_j} p_{\theta'}(a_j|f_j,e_{a_j}) \, \bigtriangledown log \, p_{\theta}(f_j | e_{a_j}) \quad + p_{\theta'}(a_j, a_{j-1}|f_j,e_{a_j}) \, \bigtriangledown log \, p_{\theta}(a_j | a_{j-1})  ] $

\item We choose $a$ is state, $f$ is observation. We remove $e$.
Therefore, \\

\begin{equation}
J(\theta) = \sum_{j=1}^J \sum_{a_j} p_{\theta'}(a_j|f_j) \, \bigtriangledown log \, p_{\theta}(f_j | e_{a_j}) \quad + p_{\theta'}(a_j, a_{j-1}|f_j) \, \bigtriangledown log \, p_{\theta}(a_j | a_{j-1})  ] 
\label{Word Alignment Model Neural Network Gradient}
\end{equation}

\end{enumerate}

\paragraph{How to apply NN ?}
\begin{itemize}
\item Emission matrix $p_{\theta}(f_j | e_{a_j})$: It is not the normal way of translation. We receive the target-language sentence and return the probabilities for all source-language vocabulary.
\item Transition matrix $p_{\theta}(a_j | a_{j-1})$: We receive the target-language sentence and return this matrix.
\item The posteriors $p_{\theta'}(a_j|f_j)$ and $p_{\theta'}(a_j, a_{j-1}|f_j)$ : We set source-language sentence as a set of observation, Baum-Welch algorithm returns forward and backward messages (How is the size of these messages)
\end{itemize}

\paragraph{What is the problem of transition matrix in programming ?}
The input of transition model is target sentence. The model returns the matrix with its size being ($I * I$). 

However, this matrix is hard to return. This is why I return matrix ($I * V_e$) with $V_e$ is the size of target language vocabulary. From this matrix, I could select the probabilities of the words existed in target sentence. The final result is a matrix with its size being ($I * I$).

This method breaks the structure of the tensor tree. The cost calculated can not be back-propagated because there is no relationship between this cost and this matrix ($I * V_e$).

\subsection{Non-probability - Score approach}
\subsubsection{\cite{Yang13word}, \cite{Tamura14recurrent}}



\section{Symmetrical alignment}


\section{Techniques in Alignment}

\subsection{Agreement between models}

\section{Evaluation of a translation/alignment}
\begin{enumerate}
\item Alignment error rate
\item Attention concentration: Entropy \citep{Ghader2017what}, Absentmindedness penalty \citep{Rikters2017confidence}
\item Attention loss: Compared with known soft alignment
\item Confidence metric - Auto-evaluation: Coverage deviation penalty and Absentmindedness penalty \citep{Rikters2017confidence}

\end{enumerate}

\section{HMM-NN Experiment}
\subsection{Model 1: Simple NN-based Emission}
We review the equation \eqref{Word Alignment Model Neural Network Gradient}.

$
J(\theta) = \sum_{j=1}^J \sum_{a_j} p_{\theta'}(a_j|f_j) \, \bigtriangledown log \, p_{\theta}(f_j | e_{a_j}) \quad + p_{\theta'}(a_j, a_{j-1}|f_j) \, \bigtriangledown log \, p_{\theta}(a_j | a_{j-1})  ] 
$

becomes

\begin{equation}
J(\theta) = \sum_{j=1}^J \sum_{a_j} p_{\theta'}(a_j|f_j) \, \bigtriangledown log \, p_{\theta}(f_j | e_{a_j}) \quad
\end{equation}


\begin{itemize}
\item \textbf{Emission matrix }$p_{\theta}(f_j | e_{a_j})$: We receive the target-language sentence and return the probabilities for all source-language vocabulary under Feed-Forward NN.

\item \textbf{Emission posterior} $p_{\theta'}(a_j|f_j)$ : We set source-language sentence as a set of observation, Baum-Welch algorithm returns forward and backward messages with size (J * I)

\item \textbf{Transition matrix} $p_{\theta}(a_j | a_{j-1})$: This matrix does not depend on the source-language and the target-language sentence. It depends on a set of jump distance.
	\begin{enumerate}
	\item Initialize jump distance set.
	\item Create transition matrix T with size (I * J) from this jump distance set.
	\item Create initial transition matrix  T0 with size (1 * I) randomly
	\item Calculate forward and backward message from NN-based emission matrix, transition matrix T and intial transition matrix T0.
	\item Emission posterior is forward * backward message (See\cite{Tran16unsupervised})
	\end{enumerate}
\end{itemize}


\begin{algorithm}[H]
\textbf{Input:}\\
	\begin{itemize}
	\item Source sentence: $f_1^J = f_1,...,f_j,...f_J$
	\item Target sentence: $e_1^I = f_1,...,f_i,...f_I$
	\end{itemize}
\textbf{Initialization:} \\
	\begin{itemize}
	\item Initialize jump distance set JD
	\end{itemize}
\textbf{Training:} \\
\While{each sentence pair $(f_1^J, e_1^I)$ in Training set}{
	\begin{enumerate}
	\item Get Emission matrix E with size $(I * V_{F})$ from NN
	\item Get Transition matrix T $(I * I)$ from \textbf{JD}
	\item Select randomly Initial transition matrix T0 with $(1 * I)$
	\item Get forward $\alpha$ and backward $\beta$ message with size $(J*I)$
	\item Get Emission posterior $EP = \alpha * \beta $, Normalize EP and Reshape EP with size $(I * V_{F})$
	\item Get expected-T $(I * I)$ from E, T, $\alpha$, $\beta$
	\item Update \textbf{JD} from expected-T
	\end{enumerate}
	
 }
 \textbf{Validation:} \\
 \While{each sentence pair $(f_1^J, e_1^I)$ in Validation set}{
	\begin{enumerate}
	\item Get Emission matrix E with size $(I * V_{F})$ from NN
	\item Get Transition matrix T $(2I * 2I)$ with Null tokens from \textbf{JD}
	\item Select randomly Initial transition matrix T0 with $(1 * 2I)$
	\item Get location pair $(J * 2I)$ with Viterbi from E with size $(2I * V_{F})$ and T with $(2I * 2I)$
	\item Calculate AER
	\end{enumerate}
	
 }
 
 \caption{Model 1}
\end{algorithm}


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\pagebreak 
\bibliographystyle{plain}
\bibliography{Khoa_Alignment_Paper}

%----------------------------------------------------------------------------------------


\end{document}
