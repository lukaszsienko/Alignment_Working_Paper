\documentclass{article}

\usepackage{graphicx} % Required for the inclusion of images
\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 

\setlength\parindent{0pt} % Removes all indentation from paragraphs


%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Paper Report}
\author{NGO HO Anh Khoa}

\begin{document}
\maketitle

\tableofcontents
\newpage

% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}


\part{Paper report}

%----------------------------------------------------------------------------------------
%	IBM models
%----------------------------------------------------------------------------------------
IBM models


\section{IBM models}

%----------------------------------------------------------------------------------------
%	Unsupervised Neural HMMs
%----------------------------------------------------------------------------------------

\section{Unsupervised Neural HMMs \citep{Tran16unsupervised}}
\subsection{Main idea}
This research show how to apply unsupervised hidden Markov model in neural network approach. In fact, they would like to prove that a simple nn models trained to maximize the marginal likelihood could outperform more complicated models in unsupervised learning.

\subsection{What is their concentration ?}
\begin{itemize}
\item There are three components:
	\begin{itemize}
	\item Set of latent variables Z (Tags)
	\item Set of observed variables X (Words)
	\item Model parameters $\theta$ (Emission and transitions probability)
	\end{itemize}

\item Purpose: Find $\theta$ which maximize p(X $|\theta$)
\item How: Use Generalized EM to estimate $\theta$
	\begin{enumerate}
	\item : Maximizing $p(X)$ means
	\begin{equation}
	p(x) = \sum_{z} p(X,Z) = E_{q(Z)}[\ln p(X,Z|\theta)] + H[q(Z)] + KL(q(Z) || p(Z|X,\theta))
	\end{equation}
	\item E-step: Estimate $p(Z|X)$ based on current $\theta$
	\item M-step: Consider $q(Z) = p(Z|X)$ \\
	$KL(q(Z) || p(Z|X,\theta)) = 0$ \\
	$H[q(Z)]$ constant
	\item Maximizing $p(X)$ becomes maximizing $E_{q(Z)}[\ln p(X,Z|\theta)]$
	\end{enumerate}
\item Result: Gradient of the joint probability scaled by the posteriors
	\begin{equation}
	J(\theta) = \sum_{Z} p(Z|X) \dfrac{\partial}{\partial\theta} \ln p(X,Z| \theta)
	\label{GradientJointProbability} 
	\end{equation}
\item Problem: How to calculate $p(X,Z)$ ?
\end{itemize}

\subsection{What is the role of HMM ?}
\begin{itemize}
\item Assumption:
	\begin{itemize}
	\item Every word token is generated by a latent class (Tag)
	\item The current class at time t is conditioned on the previous class at time (t - 1)
	\end{itemize}

\item Therefore, the probability of a given sequence of observation X and latent variables Z (Factorization of the joint probability):
	\begin{equation}
	p(X,Z) = \prod_{t=1}^{n + 1} p(z_{t}|z_{t-1})\prod_{t=1}^{n} p(x_{t}|z_{t})
	\label{ProbabilityHMM}
	\end{equation}
	
\item Result: Combine $p(X,Z)$ \eqref{ProbabilityHMM} and gradient $J(\theta)$ \eqref{GradientJointProbability}

$J(\theta) = \sum_{Z} p(Z|X) \dfrac{\partial}{\partial\theta} \ln p(X,Z| \theta)$ \\

$ = \sum_{Z} p(Z|X) \dfrac{\partial}{\partial\theta} \ln [\prod_{t=1}^{n + 1} p(z_{t}|z_{t-1},\theta)\prod_{t=1}^{n} p(x_{t}|z_{t},\theta) ]$ \\

$ = \sum_{Z} p(Z|X) \dfrac{\partial}{\partial\theta}  [\sum_{t=1}^{n + 1} \ln p(z_{t}|z_{t-1},\theta) + \sum_{t=1}^{n} \ln p(x_{t}|z_{t},\theta) ]$

$ = \sum_{t}\sum_{z_{t}} p(z_{t}, z_{t-1}|X) \dfrac{\partial}{\partial\theta} \ln p(z_{t}|z_{t-1},\theta) + p(z_{t}|X) \dfrac{\partial}{\partial\theta} \ln p(x_{t}|z_{t},\theta) $

\begin{equation}
	J(\theta) = \sum_{t}\sum_{z_{t}} p(z_{t}, z_{t-1}|X) \dfrac{\partial}{\partial\theta} \ln p(z_{t}|z_{t-1},\theta) + p(z_{t}|X) \dfrac{\partial}{\partial\theta} \ln p(x_{t}|z_{t},\theta)
\end{equation}

\item Problem:
	\begin{itemize}
	\item How to calculate $p(z_{t}, z_{t-1}|X)$ and $p(z_{t}|X)$ ? They propose Baum-Welch
	\item How to calculate $p(z_{t}|z_{t-1},\theta)$ and $p(x_{t}|z_{t},\theta)$ ? They propose Neural Networks
	\end{itemize}
\end{itemize}

\subsection{Where is neural network ?}
\begin{itemize}
\item Input: A sentence $X = x_{1},..., x_{t},... x_{L_{x}}$, a set of vocabulary $W = w_{1},..., w_{i},... w_{L_{W}}$ a set of tags $Z = z_{1},..., z_{j},... z_{L_{z}}$
\item Output: $p(z_{t}|z_{t-1},\theta)$ and $p(x_{t}|z_{t},\theta)$ at each time t.

\item How:
	\begin{enumerate}
	\item Embedding X and Z by $\theta$: Vector embedding of W $v_{W}$ (Using CNN - Convolution for Morphology) and vector embedding of Z $v_{Z}$ (Simple feed-forward nn having a lookup table following by a non-linear activation ReLU). $v_{W}$ and $v_{Z}$ have the same dimension.
	\item Calculate $p(x_{t}|z_{t},\theta)$ (Emission matrix): Probability of a word $w_{i}$ in Vocabulary is generated by a tag $z_{j}$ (Do not care about time t).
	\begin{equation}
	p(w_{i}|z_{j}) = \frac{exp(v_{z_{j}}^{T}*v_{w_{i}} + b_{i})}{\sum_{w \in W} exp(v_{z_{j}}^{T}*v_{w} + b)}
	\end{equation}

	\item Calculate $p(z_{t}|z_{t-1},\theta)$ (Transition matrix): Probability of a tag $z_{j}$ at time t is generated by a tag $z_{j'}$ at time (t - 1).\\
	Input: Vector of word w at $x_{t}$ noted $v_{x_{t}}$. It is query embedding (Using LSTMs) \\
	Result: Matrix of $L_{z}*L_{z}$ noted T. It means all transition probabilities of each tag at (t - 1) to all tags at time t.\\
	\begin{equation}
	T = U^{T}*v_{x_{t}} + b
	\end{equation}
	\end{enumerate}
\end{itemize}

%----------------------------------------------------------------------------------------
%	What does Attention in NMT pay Attention to ?
%----------------------------------------------------------------------------------------

\section{What does Attention in NMT pay Attention to ? \citep{Ghader2017what} }
\subsection{Main idea}
This research compares between Attention Models (Non-recurrent attention model/ Global attention and Recurrent attention/ Input-feeding model) and known Word Alignment.
The result is that their differences depends on the word type being generated.
\subsection{How to compare ?}
Higher consistency between Attention and Alignment leading to better translation.

\begin{itemize}
\item \textbf{Spearman's rank correlation between attention quality} (Attention loss compared known human alignment) \textbf{and translation quality} (Word prediction loss).
Higher correlation means a closer relationship between translation quality and consistency of attention versus alignment.
	\begin{itemize}
	\item Attention loss
		\begin{equation}
		L_{At}(outToken) = - \sum_{inToken} Al(in, out) * log(At(in, out))
		\end{equation}
		\begin{itemize}
		\item Al(in,out): Weight of alignment link between input token and output token
		\item At(in,out): Weight of attention between input token and output token
		\end{itemize}

	\item Word prediction loss: Softmax()
	\end{itemize}
	
\item Attention concentration: Entropy of attention distribution (Soft-hard attention problem)
	\begin{equation}
	E_{At}(outToken) = - \sum_{in} At(in, out) * log (At(in, out))
	\end{equation}
	\begin{itemize}
	\item At(in,out): Weight of attention between input token and output token
	\end{itemize}
\end{itemize}
\subsection{Analysis}
The analysis is based on POS tags experiments.
\begin{itemize}
\item Impact of Attention between Non-recurrent (NR) and Recurrent attention (IF): IF has lower AER (Hard attention) and Attention loss (Soft attention).
\item Translation quality: Consistency between attention and word alignment depends on POS tags. For example,
	\begin{itemize}
	\item There is a higher consistency in the case of nouns. However, attention captures other information in the case of verbs.
	\item Translation quality of Verbs is better than Nouns. It means attention does not follow alignment for translating Verbs.
	\end{itemize}
\item Attention concentration: Review the case of Verbs and Nouns.
	\begin{itemize}
	\item Nouns have a lower attention entropy (Higher concentration), lower attention loss (Closer to Alignment), which is that attention entropy can be used as a measure of closeness of attention to alignment in the case of nouns
	\item Verbs have a lower correlation between attention entropy and word prediction loss, which means that attention concentration is not necessary for translating verbs.
	\end{itemize}
\item Attention distribution: It shows how a POS tag of target sentence depends on other POS tags of source sentence.
\end{itemize}

%----------------------------------------------------------------------------------------
%	What does Attention in NMT pay Attention to ?
%----------------------------------------------------------------------------------------

\section{Confidence through Attention \citep{Rikters2017confidence}}

\subsection{Main idea}
\textbf{Auto-evaluation metric without reference.}
This research is that attention distribution becomes a confidence metric (Translation quality and Decoder confidence)
\begin{itemize}
\item Filtering out bad translation from a large back-translated corpus (Provide a better parallel corpus)
\item Selecting the best translation in a hybrid setup of 2 translation systems
\end{itemize}
The result is that this metric could be consider as an human judgement (Not so true!, just about 50\%) and leads to BLEU score improvement (in some cases).

\subsection{How to calculate Confidence metric ?}
Penalty measures: Coverage deviation and Absentmindedness
\begin{itemize}
\item Coverage Deviation Penalty: Lacking attention and Too much attention per input token, which mean penalizing the sum of attention per input token for going to far from 1.0 (Why 1.0: Replaced by token's expected fertility)

\begin{equation}
CDP = -\frac{1}{inSentLen}\sum_{inToken} log(1 + (1 - \sum_{outToken} \alpha_{out-inToken})^2 )
\end{equation}

\item Absentmindedness Penalty (Entropy): The attention of confident output tokens should concentrate on a small number of input tokens and vice versa (Assumption).
\begin{equation}
AP_{out} = -\frac{1}{inSentLen}\sum_{inToken} \sum_{outToken} \alpha_{out-inToken} * log (\alpha_{out-inToken})
\end{equation}

\item Combination:
\begin{equation}
Confidence = CDP + AP_{out} + AP_{in}
\end{equation}
\end{itemize}

\subsection{Analysis}
\subsubsection{Comparison with human evaluation}
They use Kendall rank correlation coefficient for looking at the pairs where human scores differ. They recognize that their metric over-penalizes the translations which do not follow the source word-by-word.
\subsubsection{Exp: Filtering Back-translated Data}
They compare their confidence metric with language model method in filtering the best translated sentences. Both methods have the similar levels of overlapping the human evaluation. One point should be considered is that their metric does not require any additional model (LM).

For BLEU score, their method show a better performance on some cases, which is in general insignificant. 

\subsubsection{Exp: Hybrid Decisions}
The difference between two baseline systems influences on the final BLEU score. A small difference leads to the small improvements, a large difference causes a score drop. It is well-reported that hybrid selection overlaps about 50\% human selection.

%----------------------------------------------------------------------------------------
%	Word Translation without Parallel data
%----------------------------------------------------------------------------------------

\section{Word Translation without Parallel data \citep{Conneau2017Word}}

\subsection{Main idea}
The research is about \textbf{building a bilingual dictionary without parallel data} by aligning monolingual word embedding spaces in a unsupervised way (GAN) and proposing a similarity metric CSLS.

Its results show a strong performance of using Procrustes-CSLS. GAN in this case is a step necessary to overcome unsupervised learning. 

\subsection{How does it works ?}
Input: Two large monolingual corpora.\\
Output: Linear mapping W between the source and target space.
Two steps of training:
\begin{enumerate}
\item Training GAN:
	\begin{itemize}
	\item A discriminator distinguishes between n mapped source embeddings and m mapped target embeddings.\\
	Objective function:
	\begin{equation}
	Loss_{D}(\theta_{D}| W) = - \frac{1}{n} \sum_{i=1}^{n} \log P_{\theta_{D}}(source=true|Wx_{i}) - \frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_{D}}(source=false|y_{i})
	\end{equation}
	
	\item A generator creates these embeddings to fool discriminator.\\
	Objective function:
	\begin{equation}
	Loss_{W}(W|\theta_{D}) = - \frac{1}{n} \sum_{i=1}^{n} \log P_{\theta_{D}}(source=false|Wx_{i}) - \frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_{D}}(source=true|y_{i})
	\end{equation}
	
	\item Update parameter:\\
	Orthogonality advantages: Reservation of monolingual quality of the embeddings (Dot product of vectors or distances, rotation $\rightarrow$ An isometry of the Euclidean space); Stable training)
	\begin{equation}
	W \leftarrow (1 + \beta)W - \beta(WW^{T})W
	\end{equation}
	
	
	\end{itemize}
\item Refinement procedure (Solution for rare words that GAN does not well solve) repeats until reaching stopping condition.
	\begin{enumerate}
	\item Extracting a synthetic high-quality dictionary (Most frequent words) evaluated by CSLS
	\item Applying Procrustes solution for generating more accurate dictionary.
	\end{enumerate}
\item Stopping condition/Best hyper-parameters selection - Validation step: Similarity measure CSLS between mapped source and target words.\\
	Why: It is based on K-NN and overcomes a problem of two spaces and "Hubs and Anti-hubs" (Some points are highly near many other points while there are some points are not nearest any point in high-dimensional spaces).\\
	They proposed a bipartite neighbourhood graph. Each word of a language is connected to K words of an other language.
	\begin{equation}
	CSLS(Wx_{s}, y_{t}) = 2 \cos(Wx_{s}, y_{t}) - r_{T}(Wx_{s}) - r_{S}(y_{t})
	\end{equation}
		\begin{itemize}
		\item $r_{T}(Wx_{s}) = \frac{1}{K} \sum_{y_{t} \in N_{T}(Wx_{s})} \cos(Wx_{s}, y_{t})$\\
		Mean similarity of a source embedding $x_{s}$ to its target neighbours.
		\item $r_{S}(y_{t}) = \frac{1}{K} \sum_{Wx_{s} \in N_{S}(y_{t})} \cos(y_{t}, Wx_{s})$\\
		Mean similarity of a source embedding $x_{s}$ to its target neighbours.
		\item $N_{T}(Wx_{s})$: Neighbour of a source word $Wx_{s}$ in target word space.
		\end{itemize}

\end{enumerate}

%----------------------------------------------------------------------------------------
%	Word Alignment Modeling with Context Dependent Deep Neural Network
%----------------------------------------------------------------------------------------

\section{Word Alignment Modeling with Context Dependent Deep Neural Network \citep{Yang13word}}

\subsection{Main idea}
The research describes how to use Context Dependent Deep NN in HMM-based word alignment model. This means that bilingual word embedding could capture not only lexical translation and context from surrounding words.
An example is translating a rare word by using its surrounding words.\\
It is noted that this version uses a smaller number of parameters than the classic HMM model.

\subsection{How does DNN work in word alignment ?}
The role of DNN in this paper is learning automatically feature from raw text.
They start from the factorization of the joint probability \cite{Vogel1996HMM}.
\begin{equation}
p(a, e| f) = \prod_{i = 1}^{|e|} P_{lex}(e_{i}|f_{a_{i}}) P_{d}(a_{i}|a_{i-1})
\end{equation}
	\begin{itemize}
	\item Given a sentence pair (e,f) 
	\item $P_{lex}$: Lexical translation probability, emission probability
	\item $P_{d}$: HMM alignment probability \cite{Vogel1996HMM}, transition probability. The paper has a different notation: $P_{d}(a_{i} - a_{i-1})$ and it is called Jump distance distortion probability (Why: The alignment probability depends only on the jump width $(a_{i} - a_{i-1})$ \cite{Vogel1996HMM}).
	\end{itemize}

They propose using a score instead of these probabilities because they would like to avoid the softmax normalization step of a large vocabulary. The formula above becomes:

\begin{equation}
s_{NN}(a|e, f) = \prod_{i = 1}^{|e|} t_{lex}(e_{i},f_{a_{i}}|e,f) t_{d}(a_{i},a_{i-1}|e,f)
\end{equation}
	\begin{itemize}
	\item $f_{a_{i}}$ represents not only this word but the context around this word. Surrounding words of both source and target word are input of DNN, which is handling the context of both sides. This could reduce the explosion of parameter number. \\ In this case, they use fixed length windows surrounding both $e_{i}$ and $f_{j}$. 
	\item $t_{lex}$: Lexical translation score.
	\begin{equation}
	t_{lex}(e_{i},f_{j}|e,f) = functions_{NN}(window(e_{i}),window(f_{j}) )
	\end{equation}
	\item $t_{d}$: Distortion score.
	\begin{equation}
	t_{d}(a_{i}, a_{i-1}|e,f) = t_{d}(a_{i} - a_{i-1}|window(f_{a_{i-1}})) = functions_{NN}(window(f_{a_{i-1}}))
	\end{equation}
	They recognize that this lexicalized distortion does not produce a better alignment. They reverse to the simple version.
	\begin{equation}
	t_{d}(a_{i}, a_{i-1}|e,f) = t_{d}(a_{i} - a_{i-1})
	\end{equation}
	\end{itemize}

%----------------------------------------------------------------------------------------
%	Report
%----------------------------------------------------------------------------------------
\pagebreak 
\part{Report}

\section{Evaluation of a translation/alignment}
\begin{enumerate}
\item Alignment error rate
\item Attention concentration: Entropy \citep{Ghader2017what}, Absentmindedness penalty \citep{Rikters2017confidence}
\item Attention loss: Compared with known soft alignment
\item Confidence metric - Auto-evaluation: Coverage deviation penalty and Absentmindedness penalty \citep{Rikters2017confidence}

\end{enumerate}


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\pagebreak 
\bibliographystyle{plain}
\bibliography{Khoa_Alignment_Paper}

%----------------------------------------------------------------------------------------


\end{document}
